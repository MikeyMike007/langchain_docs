{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Routing in LangChain Expression Language**:\n",
    "  - The concept of routing is explored in LangChain Expression Language, allowing for the creation of non-deterministic chains.\n",
    "  - In these chains, the output of one step determines the subsequent step, providing a structured and consistent approach to interactions with LLMs.\n",
    "\n",
    "- **Methods for Implementing Routing**:\n",
    "  1. **Using RunnableBranch**: This method involves utilizing a `RunnableBranch` for routing purposes.\n",
    "  2. **Custom Factory Function**: Alternatively, routing can be achieved by writing a custom factory function. This function should take the output of a previous step and return a runnable without executing it.\n",
    "\n",
    "- **Practical Example of Routing**:\n",
    "  - An example showcases a two-step sequence:\n",
    "    - The first step involves classifying an input question as related to LangChain, Anthropic, or Other.\n",
    "    - Based on this classification, the sequence is then routed to a corresponding prompt chain tailored to the identified category.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a `RunnableBranch`\n",
    "\n",
    "**Concept of RunnableBranch in LangChain**:\n",
    "  - A `RunnableBranch` is initialized with a list of pairs, each consisting of a condition and a corresponding runnable, along with a default runnable.\n",
    "  - The selection of the branch is based on evaluating each condition with the given input.\n",
    "  - The first condition that evaluates to True triggers the execution of its associated runnable.\n",
    "  - If none of the provided conditions match, the default runnable is executed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chain Creation and Functionality**\n",
    "\n",
    "- Different types of chains (Classification, LangChain, Anthropic, General) were created using `PromptTemplate`, `ChatOpenAI`, and `StrOutputParser`.\n",
    "- Each chain is designed to handle specific types of questions or topics.\n",
    "\n",
    "**Branching Logic with RunnableBranch**\n",
    "\n",
    "- The `RunnableBranch` uses lambda functions to route input to the appropriate chain based on topic classification.\n",
    "\n",
    "**Full Chain Composition and Invocation**\n",
    "\n",
    "- The `full_chain` combines the classification chain with branching logic.\n",
    "- The `invoke` method is used to pass a question to the chain for processing and response.\n",
    "\n",
    "**Simplification with RunnablePassthrough**\n",
    "\n",
    "- `RunnablePassthrough` was suggested to streamline the chain by passing the entire input dictionary directly to the next step.\n",
    "\n",
    "**Role of Lambda Functions**\n",
    "\n",
    "- Lambda functions act like callback functions, executing at the right moment in the chain’s execution to process input.\n",
    "- They are necessary for dynamic handling of input within the chain.\n",
    "\n",
    "**Passing Dictionaries in LangChain**\n",
    "\n",
    "- In LangChain chains, dictionaries are used to pass data between different components of the chain. Each component may modify or add to this dictionary.\n",
    "\n",
    "**Function of Lambda Functions**\n",
    "\n",
    "- Lambda functions (`lambda x: ...`) are used to manipulate or access data within these dictionaries. They act as callback functions, executed at specific points in the chain.\n",
    "- For example, `lambda x: x[\"question\"]` extracts the `\"question\"` value from the input dictionary.\n",
    "\n",
    "**Use of RunnablePassthrough**\n",
    "\n",
    "- `RunnablePassthrough` is a simplification tool that forwards the entire input dictionary to the next component without any modification.\n",
    "- It's useful when the entire context needs to be passed along the chain.\n",
    "\n",
    "**Invoking Chains with Dictionaries**\n",
    "\n",
    "- When invoking a chain (e.g., `full_chain.invoke(...)`), the input is typically a dictionary (e.g., `{\"question\": \"how do i use langchain?\"}`).\n",
    "- This dictionary becomes the input for the first component of the chain.\n",
    "\n",
    "**Branching Logic and Dictionary Access**\n",
    "\n",
    "- In branching logic (like `RunnableBranch`), lambda functions check specific keys (like `\"topic\"`) in the input dictionary to decide the flow of execution.\n",
    "- The dictionary structure allows for flexible and dynamic handling of input based on the conditions defined in these lambda functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models.openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    PromptTemplate.from_template(\n",
    "        \"\"\"Given the user question below, classify it as either being about\n",
    "        `LangChain`, `Anthropic`, or `Other`.\n",
    "        \n",
    "        Do not respond with more than one word.\n",
    "        \n",
    "        <question> {question} </question>\n",
    "        \n",
    "        Classification:\"\"\"\n",
    "    )\n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Anthropic'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\n",
    "  {\n",
    "    \"question\": \"how do i call Antrhopic?\"\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_chain = (\n",
    "    PromptTemplate.from_template(\n",
    "        \"\"\"You are an expert in langchain. \\\n",
    "          Always answer questions starting with \"As Harrison Chase told me\". \\\n",
    "          Respond to the following question:\n",
    "        \n",
    "          Question: {question} Answer:\"\"\"\n",
    "    )\n",
    "    | ChatOpenAI()\n",
    ")\n",
    "\n",
    "anthropic_chain = (\n",
    "    PromptTemplate.from_template(\n",
    "        \"\"\"You are an expert in anthropic. \\\n",
    "          Always answer questions starting with \"As Dario Amodei told me\". \\\n",
    "          Respond to the following question:\n",
    "        \n",
    "          Question: {question} Answer:\"\"\"\n",
    "    )\n",
    "    | ChatOpenAI()\n",
    ")\n",
    "\n",
    "general_chain = (\n",
    "    PromptTemplate.from_template(\n",
    "        \"\"\"Respond to the following question:\n",
    "        \n",
    "          Question: {question} Answer:\"\"\"\n",
    "    )\n",
    "    | ChatOpenAI()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableBranch\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "branch = RunnableBranch(\n",
    "    (lambda x: \"anthropic\" in x[\"topic\"].lower(), anthropic_chain),\n",
    "    (lambda x: \"langchain\" in x[\"topic\"].lower(), langchain_chain),\n",
    "    general_chain\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_chain = {\"topic\": chain, \"question\": lambda x: x[\"question\"]} | branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='As Harrison Chase told me, to use Langchain, you first need to create an account on their website. Once you have an account, you can access their platform and start using their language learning features. They offer various resources such as interactive lessons, vocabulary exercises, and language exchange opportunities. You can track your progress, set goals, and even connect with language tutors for personalized guidance. Langchain also provides a community forum where you can interact with other language learners and seek help when needed. So, start by signing up on their website and explore the different tools and features they offer to enhance your language learning journey.')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke(\n",
    "  {\n",
    "    \"question\": \"how do i use langchain?\"\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"As Dario Amodei told me, anthropic is a framework for understanding and reasoning about artificial general intelligence (AGI) and its impact on the world. To use anthropic, you can start by familiarizing yourself with its principles and theories, which aim to address AGI's potential risks and align it with human values. You can study the various papers and writings by Dario Amodei and his colleagues to gain a deeper understanding. Additionally, engaging in discussions and collaborating with the anthropic community can provide valuable insights and perspectives.\")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke(\n",
    "  {\n",
    "    \"question\": \"how do i use anthropic?\"\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The answer to 2 + 2 is 4.')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke(\n",
    "  {\n",
    "    \"question\": \"whats 2 + 2?\"\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a custom function\n",
    "\n",
    "You can also use a custom function to route between different outputs. Here’s an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route(info):\n",
    "  if \"anthropic\" in info[\"topic\"].lower():\n",
    "    return anthropic_chain\n",
    "  elif \"langchain\" in info[\"topic\"].lower():\n",
    "    return langchain_chain\n",
    "  else:\n",
    "    return general_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (612748699.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[30], line 7\u001b[0;36m\u001b[0m\n\u001b[0;31m    | RunnableLambda(full_chain)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "full_chain = {\n",
    "  \"topic\": chain,\n",
    "  \"question\": lambda x: x[\"question\"]\n",
    "}\n",
    "| RunnableLambda(route)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_docs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
