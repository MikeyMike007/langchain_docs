{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Expression Language\n",
    "\n",
    "- **Overview of LangChain Expression Language (LCEL)**:\n",
    "  LCEL provides a declarative method for composing chains in LangChain. It's designed for ease of transitioning prototypes to production, supporting everything from simple \"prompt + LLM\" chains to complex ones with hundreds of steps used in production environments.\n",
    "\n",
    "- **Key Features of LCEL**:\n",
    "  - **Streaming Support**: LCEL allows for efficient streaming, minimizing time-to-first-token. This means faster and incremental outputs directly from an LLM to a streaming output parser.\n",
    "  - **Async Support**: Chains built with LCEL can be executed using both synchronous (e.g., in Jupyter notebooks for prototyping) and asynchronous APIs (e.g., in LangServe servers), facilitating seamless transition from prototype to production.\n",
    "  - **Optimized Parallel Execution**: LCEL automatically executes parallel steps in a chain, reducing latency in both synchronous and asynchronous modes.\n",
    "  - **Retries and Fallbacks**: It offers configurable retries and fallbacks for any part of a chain, enhancing reliability at scale. Future developments include streaming support for these features to maintain low latency.\n",
    "  - **Intermediate Results Access**: Useful in complex chains, this feature allows monitoring of intermediate step results for user reassurance or debugging. It's available in every LangServe server.\n",
    "  - **Input and Output Schemas**: LCEL chains have Pydantic and JSONSchema schemas, inferred from the chain's structure, useful for input and output validation.\n",
    "  - **Integration with LangSmith Tracing**: As chains grow in complexity, LangSmith tracing provides vital observability and debugging capabilities, with automatic logging of each step in a chain.\n",
    "  - **Seamless LangServe Deployment**: Chains created with LCEL can be easily deployed using LangServe, streamlining the deployment process.\n",
    "\n",
    "LCEL's design and features make it highly suitable for both prototyping and scaling language model applications in production environments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Example\n",
    "\n",
    "- **Using the Pipe Operator (|) in LCEL**:\n",
    "  - The pipe symbol (`|`) functions similarly to a Unix pipe, connecting different components.\n",
    "  - It allows for the output of one component to seamlessly feed into the next, creating an integrated workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the ice cream go to therapy? It had too many sprinkles of anxiety!'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "  \"\"\"\n",
    "  tell me a short joke about {topic}\n",
    "  \"\"\"\n",
    ")\n",
    "\n",
    "model = ChatOpenAI()\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "chain.invoke({\"topic\": \"ice cream\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt\n",
    "\n",
    "- **Functionality of BasePromptTemplate**:\n",
    "  - A `BasePromptTemplate` in LangChain is designed to receive a dictionary of template variables and generate a `PromptValue`.\n",
    "  - The `PromptValue` is essentially a completed prompt that is adaptable for use with different types of language models.\n",
    "\n",
    "- **Adaptability with Language Models**:\n",
    "  - The `PromptValue` is versatile and can interface with either an LLM (Language Learning Model) or a ChatModel.\n",
    "  - For LLMs, which require a string input, it produces a string.\n",
    "  - For ChatModels, which take a sequence of messages as input, it generates `BaseMessages`.\n",
    "  - This adaptability is due to the `BasePromptTemplate` having the logic to produce both `BaseMessages` and a string output.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "  \"\"\"\n",
    "  tell me a short joke about {topic}\n",
    "  \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='\\n  tell me a short joke about ice cream\\n  ')])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_value = prompt.invoke({\"topic\": \"ice cream\"})\n",
    "prompt_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='\\n  tell me a short joke about ice cream\\n  ')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_value.to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: \\n  tell me a short joke about ice cream\\n  '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_value.to_string()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "The PromptValue is then passed to model. In this case our model is a ChatModel, meaning it will output a BaseMessage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Why did the ice cream go to therapy? Because it had too many sprinkles of anxiety!')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ChatOpenAI()\n",
    "message = model.invoke(prompt_value)\n",
    "message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our model instead was a LLM (compared to a ChatModel), then output will be a string (instead of a BaseMessage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nRobot: Why don't penguins like ice cream?  Because they can't get the wrapper off.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI()\n",
    "llm.invoke(prompt_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Parser\n",
    "\n",
    "- **Function of BaseOutputParser in Model Output Processing**:\n",
    "  - In LangChain, the output from a model is typically passed to a `BaseOutputParser`.\n",
    "  - A `BaseOutputParser` is designed to handle different types of inputs: either a string or a `BaseMessage`.\n",
    "\n",
    "- **Role of StrOutputParser**:\n",
    "  - The `StrOutputParser`, a specific type of `BaseOutputParser`, simplifies the process by converting any input, regardless of its original form, into a string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the ice cream go to therapy? Because it had too many sprinkles of anxiety!'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser.invoke(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entire pipleline\n",
    "\n",
    "- **Step-by-Step Process of Chain Execution**:\n",
    "  1. **Input**: The process begins with user input in the form of a dictionary, such as `{\"topic\": \"ice cream\"}`.\n",
    "  2. **Prompt Template Processing**: The input is used by the prompt component to construct a `PromptValue`. This is done by incorporating the topic into the prompt.\n",
    "  3. **Model Evaluation**: The generated `PromptValue` is then fed to the `ChatModel`, specifically an OpenAI LLM, for processing. The output from the model is a `ChatMessage` object.\n",
    "  4. **Output Parsing**: The `StrOutputParser` takes the `ChatMessage` and converts it into a Python string, which is the final output from the `invoke` method.\n",
    "\n",
    "- **Component Flow**:\n",
    "  - Input: `{\"topic\": \"ice cream\"}`\n",
    "  - Components in sequence:\n",
    "    1. `PromptTemplate`\n",
    "    2. `ChatModel`\n",
    "    3. `StrOutputParser`\n",
    "  - Result: A Python string as the final output.\n",
    "\n",
    "- **Testing Intermediate Results**:\n",
    "  - It's possible to test smaller segments of the chain, like `prompt` or `prompt | model`, to observe the intermediate outputs and understand each component's contribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/2023-12-17-00-07-08.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "  \"\"\"\n",
    "  tell me a short joke about {topic}\n",
    "  \"\"\"\n",
    ")\n",
    "\n",
    "input = {\n",
    "  \"topic\": \"ice cream\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='\\n  tell me a short joke about ice cream\\n  ')])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.invoke(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Why did the ice cream go to therapy? Because it was feeling a little melty.')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(prompt | model).invoke(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Search Example\n",
    "\n",
    "- **Retrieval-Augmented Generation Chain Example**:\n",
    "  This example demonstrates how to set up a chain in LangChain for responding to questions with added context, using a retrieval-augmented generation approach.\n",
    "\n",
    "- **Setup and Components**:\n",
    "  - **Retriever Setup**: An in-memory store retriever is configured to fetch relevant documents based on a query, using `DocArrayInMemorySearch.from_texts`.\n",
    "  - **Prompt Template**: A template is created that incorporates both the context (retrieved documents) and the user's question.\n",
    "  - **Chain Components**:\n",
    "    - `ChatOpenAI` (Model)\n",
    "    - `StrOutputParser` (Output Parser)\n",
    "    - `RunnableParallel` (Handles parallel tasks)\n",
    "    - `RunnablePassthrough` (Passes the user's question)\n",
    "\n",
    "- **Chain Composition**:\n",
    "  - The chain is composed as `setup_and_retrieval | prompt | model | output_parser`.\n",
    "  - `setup_and_retrieval` uses `RunnableParallel` to prepare inputs for the prompt by retrieving documents and passing the user's question.\n",
    "  - The output of `setup_and_retrieval` feeds into the `prompt` to create a `PromptValue`.\n",
    "  - The model processes the `PromptValue` and outputs a `ChatMessage` object.\n",
    "  - The `output_parser` then converts the `ChatMessage` into a Python string.\n",
    "\n",
    "- **Process Flow**:\n",
    "  1. **Retrieval**: `retriever.invoke(\"where did harrison work?\")` fetches relevant documents.\n",
    "  2. **Input Preparation**: `RunnableParallel` combines the retrieved documents (context) and the user's question.\n",
    "  3. **Prompt Generation**: User input and context are used to create a prompt.\n",
    "  4. **Model Evaluation**: The prompt is evaluated by the model, generating a `ChatMessage`.\n",
    "  5. **Output Parsing**: The `ChatMessage` is transformed into a string by the output parser.\n",
    "\n",
    "- **Resulting Chain Workflow**:\n",
    "  - The chain efficiently combines document retrieval with user input processing and model evaluation, culminating in a structured response.\n",
    "\n",
    "![](images/2023-12-17-00-15-08.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain.schema.document import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Harrison worked at Kensho.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore = FAISS.from_texts(\n",
    "  [\n",
    "    \"harrison worked at kensho, bears like to eat honey\"\n",
    "  ],\n",
    "  embedding=OpenAIEmbeddings()\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "model = ChatOpenAI()\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "setup_and_retrieval = RunnableParallel(\n",
    "  {\n",
    "    \"context\": retriever,\n",
    "    \"question\": RunnablePassthrough()\n",
    "  }\n",
    ")\n",
    "\n",
    "chain = setup_and_retrieval | prompt | model | output_parser\n",
    "\n",
    "chain.invoke(\"where did harrison work?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': [Document(page_content='harrison worked at kensho, bears like to eat honey')],\n",
       " 'question': 'where did harrison work?'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setup_and_retrieval.invoke(\"where did harrison work?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_docs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
