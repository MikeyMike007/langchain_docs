{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buidling with langchain\n",
    "\n",
    "**Modules:** LangChain offers diverse modules for building language model applications. These modules work both as standalone entities and in combination for complex tasks.\n",
    "\n",
    "**LCEL:** Composition is enabled by the LangChain Expression Language, which allows for the integration of various modules through a unified Runnable interface.\n",
    "\n",
    "Core Components:\n",
    "\n",
    "- **LLM/Chat Model:** Fundamental reasoning engine. Understanding different language models is crucial.\n",
    "- **Prompt Template:** Key for directing the language model's output. Skills in prompt construction and strategies are essential.\n",
    "- **Output Parser:** Transforms raw language model responses into usable formats.\n",
    "- **Customization:** Most LangChain applications allow configuration of the model and prompts, important for effective use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM vs. Chat Model\n",
    "\n",
    "1. **Types of Language Models**:\n",
    "   - **LLM (Language Learning Model)**: Takes a string as input and returns a string.\n",
    "   - **ChatModel**: Accepts a list of messages as input and returns a message.\n",
    "\n",
    "2. **Message Types**:\n",
    "   - **BaseMessage Interface**: Has two required attributes:\n",
    "     - `content`: The message content, usually a string.\n",
    "     - `role`: The source of the `BaseMessage`.\n",
    "   - **Specialized Message Types**:\n",
    "     - **`HumanMessage`**: Originates from a human/user.\n",
    "     - **`AIMessage`**: Comes from an AI/assistant.\n",
    "     - **`SystemMessage`**: Generated by the system.\n",
    "     - **`FunctionMessage` / `ToolMessage`**: Contains the output from a function or tool.\n",
    "     - **`ChatMessage`**: Allows manual specification of the role.\n",
    "\n",
    "3. **Common Interface**:\n",
    "   - Both LLMs and ChatModels share a common interface, crucial for constructing appropriate prompts.\n",
    "\n",
    "4. **Method Invocation**:\n",
    "   - **`LLM.invoke()`**: Input a string, returns a string.\n",
    "   - **`ChatModel.invoke()`**: Input a list of `BaseMessage`, returns a `BaseMessage`.\n",
    "\n",
    "5. **Usage Example**:\n",
    "   - Import and create LLM and ChatModel objects.\n",
    "   - Initialize with parameters like temperature for custom configurations.\n",
    "   - Sample Invocation:\n",
    "     - LLM example: Input a text string, returns a string (e.g., a company name suggestion).\n",
    "     - ChatModel example: Input a list of messages (e.g., `HumanMessage`), returns an `AIMessage` with a response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.openai import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI()\n",
    "chat_model = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "What would be a good company name for a company making colorful socks?\n",
    "\"\"\"\n",
    "\n",
    "messages = [HumanMessage(content=text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LLM example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bright Toes Socks\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(text)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chat Model Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Rainbow Feet')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt templates\n",
    "\n",
    "1. **Role of Prompt Templates**:\n",
    "   - LLM applications typically use prompt templates instead of directly passing user input to the LLM. These templates combine user input with additional context-relevant text for specific tasks.\n",
    "\n",
    "2. **Basic Functionality**:\n",
    "   - Prompt templates simplify user interactions by handling the generation of detailed instructions for the model. An example is creating a prompt for naming a company based on a product description.\n",
    "\n",
    "3. **Advantages**:\n",
    "   - Allows partial formatting, where only some variables are formatted at a time.\n",
    "   - Supports composition, enabling the combination of multiple templates into a single prompt.\n",
    "   - Offers flexibility and advanced formatting options, detailed in the section on prompts.\n",
    "\n",
    "4. **ChatPromptTemplate**:\n",
    "   - Generates a list of messages, each with specific roles and content, suitable for chat interactions.\n",
    "   - An example usage involves creating a sequence of messages like `SystemMessage` and `HumanMessage`, each with formatted content based on the template.\n",
    "\n",
    "5. **ChatPromptTemplate Customization**:\n",
    "   - Can be constructed in various ways to suit different use cases, with more detailed explanations available in the prompts section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain_core.messages import SystemMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is a good name for a company that makes colorful socks'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}\")\n",
    "prompt.format(product=\"colorful socks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = \"You are a helpful assistant that translates {input_language} to {output_language}\"\n",
    "human_template = \"{text}\"\n",
    "\n",
    "chat_prompt_v1 = ChatPromptTemplate.from_messages([\n",
    "  (\"system\", system_template),\n",
    "  (\"human\", human_template)\n",
    "])\n",
    "\n",
    "\n",
    "# Same thing as above - Just for illustration\n",
    "chat_prompt_v2 = ChatPromptTemplate.from_messages(\n",
    "  [\n",
    "    SystemMessage(content=system_template),\n",
    "    HumanMessage(content=human_template)\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant that translates English to French'),\n",
       " HumanMessage(content='I Love Programming')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_prompt_v1.format_messages(\n",
    "  input_language=\"English\",\n",
    "  output_language=\"French\",\n",
    "  text=\"I Love Programming\"\n",
    "  \n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant that translates {input_language} to {output_language}'),\n",
       " HumanMessage(content='{text}')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_prompt_v2.format_messages(\n",
    "  input_language=\"English\",\n",
    "  output_language=\"French\",\n",
    "  text=\"I Love Programming\"\n",
    "  \n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output parsers\n",
    "\n",
    "- **Purpose of OutputParsers**:\n",
    "  OutputParsers convert the raw output from a language model into a usable format for downstream applications.\n",
    "\n",
    "- **Main Types**:\n",
    "  - Convert text from LLM into structured formats like JSON.\n",
    "  - Change a `ChatMessage` into a plain string.\n",
    "  - Transform additional data from model calls (e.g., OpenAI function invocation) into a string.\n",
    "\n",
    "- **Custom OutputParser Example**:\n",
    "  An example in the guide shows creating a custom OutputParser, `CommaSeparatedListOutputParser`, which turns a comma-separated string into a Python list. It's derived from `BaseOutputParser`, with a `parse` method that splits a string input into a list.\n",
    "\n",
    "- **Practical Use**:\n",
    "  This parser is useful for converting outputs like \"hi, bye\" into a list format `['hi', 'bye']`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import BaseOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommaSeparatedListOutputParser(BaseOutputParser):\n",
    "  \"\"\"\n",
    "  Parse the putput of an LLM call to a comma-separated list\n",
    "  \"\"\"\n",
    "\n",
    "  def parse(self, text: str):\n",
    "    return text.replace(\" \", \"\").split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi', 'bye', 'bye', 'bye']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CommaSeparatedListOutputParser().parse(\"hi, bye, bye, bye\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composing with LCEL\n",
    "\n",
    "\n",
    "- **Combining Components into a Chain**:\n",
    "  LangChain allows for combining various components into a single chain. This chain process involves taking input variables, creating a prompt via a prompt template, passing the prompt to a language model, and optionally processing the output with an output parser.\n",
    "\n",
    "- **Example Implementation**:\n",
    "  - A custom `CommaSeparatedListOutputParser` is defined to parse the language model's output into a comma-separated list.\n",
    "  - A `ChatPromptTemplate` is created with a system message template for generating lists and a human message template for user input.\n",
    "  - These components are combined into a chain: `chat_prompt | ChatOpenAI() | CommaSeparatedListOutputParser()`.\n",
    "  - Invoking the chain with an input like `{\"text\": \"colors\"}` results in an output such as `['red', 'blue', 'green', 'yellow', 'orange']`.\n",
    "\n",
    "- **Using the Pipe (|) Syntax**:\n",
    "  - The pipe (`|`) syntax, part of the LangChain Expression Language (LCEL), is used for joining these components. It leverages the universal Runnable interface that all these objects implement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = \"\"\"\n",
    "You are a helpful assistant who generates comma separated lists.\n",
    "A user will pass in a category, and you should generate 5 objects in that category in a comma separated list.\n",
    "ONLY return a comma separated list, and nothing more.\n",
    "\"\"\"\n",
    "\n",
    "human_template = \"{text}\"\n",
    "\n",
    "chat_prompt_v1 = ChatPromptTemplate.from_messages(\n",
    "  [\n",
    "    (\"system\", system_template),\n",
    "    (\"human\", human_template)\n",
    "  ]\n",
    ")\n",
    "\n",
    "chat_prompt_v2 = ChatPromptTemplate.from_messages(\n",
    "  [\n",
    "    SystemMessage(content=system_template),\n",
    "    HumanMessage(content=human_template)\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chain_v1 = chat_prompt_v1 | ChatOpenAI() | CommaSeparatedListOutputParser()\n",
    "chain_v2 = chat_prompt_v2 | ChatOpenAI() | CommaSeparatedListOutputParser() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['car', 'truck', 'motorcycle', 'bicycle', 'bus']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_v1.invoke({\"text\": \"vehicles\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_docs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
