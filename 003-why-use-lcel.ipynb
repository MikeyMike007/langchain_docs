{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why use LCEL\n",
    "\n",
    "- **LCEL Introduction and Recommendations**:\n",
    "  - LCEL (LangChain Expression Language) simplifies the construction of complex chains from basic components.\n",
    "\n",
    "- **Key Features of LCEL**:\n",
    "  1. **Unified Interface**:\n",
    "     - All LCEL objects implement the Runnable interface, ensuring a standard set of invocation methods like `invoke`, `batch`, `stream`, `ainvoke`, etc.\n",
    "     - This uniformity allows any chain of LCEL objects to also be treated as an LCEL object, supporting the same range of invocations.\n",
    "  2. **Composition Primitives**:\n",
    "     - LCEL offers various primitives for composing chains, enabling parallelization of components, adding fallbacks, dynamic configuration within chains, and more.\n",
    "\n",
    "- **Understanding LCEL's Value**:\n",
    "  - To grasp LCEL's benefits, it's useful to observe its application and consider the effort needed to replicate its functionalities without LCEL.\n",
    "  - An example walkthrough in the Get Started section demonstrates this by taking a basic prompt + model chain and exploring the underlying complexities LCEL manages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models.openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "  \"\"\"\n",
    "  Tell me a short joke about {topic}\n",
    "  \"\"\"\n",
    ")\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "model = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chain = {\"topic\": RunnablePassthrough()} | prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Why did the ice cream go to therapy? Because it couldn't stop getting sundae scaries!\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Ice cream\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='\\n  Tell me a short joke about Ice cream\\n  ')])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(({\"topic\": RunnablePassthrough()}) | prompt).invoke(\"Ice cream\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='\\n  Tell me a short joke about Ice cream\\n  ')])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.invoke({\"topic\": \"Ice cream\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Why did the ice cream go to therapy? Because it had too many sprinkles of anxiety!')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(({\"topic\": RunnablePassthrough()}) | prompt | model).invoke(\"Ice cream\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Why did the ice cream go to therapy?\\n\\nBecause it had too many toppings and couldn't keep its cool!\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(({\"topic\": RunnablePassthrough()}) | prompt | model | output_parser).invoke(\"Ice cream\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the ice cream go to therapy?\n",
      "Because it had a meltdown!"
     ]
    }
   ],
   "source": [
    "for chunk in chain.stream(\"ice cream\"):\n",
    "  print(chunk, end=\"\", flush=True)\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Why did the ice cream go to therapy? Because it was feeling a little melty!',\n",
       " 'Why did the spaghetti go to the party? Because it heard it was a pasta-tively good time!',\n",
       " 'Why did the dumpling go to the gym? \\nBecause it wanted to be a \"fit\" dumpling!']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.batch([\"ice cream\", \"spagetthi\", \"dumplings\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Why did the ice cream go to therapy? Because it had too many toppings and couldn't hold it all together!\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await chain.ainvoke(\"ice cream\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM instead of chat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI()\n",
    "\n",
    "llm_chain = {\"topic\": RunnablePassthrough()} | prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nRobot: What did the ice cream say to the unhappy sundae? \"Cheer up, scoop!\"'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.invoke(\"Ice cream\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_docs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
